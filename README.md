# Interview_questions_scenario

âœ… **Quick Headline (to remember)**
âœ… **Bullet points (what to say in interview)**
âœ… **Mini real-time story (to show impact)**

---

# **Terraform & IaC**

**1. Handling Terraform drift in production**
ğŸ‘‰ **Headline:** Detect â†’ Review â†’ Correct â†’ Apply

* Use `terraform plan -refresh-only` to detect drift.
* Example: EC2 tags manually changed by ops team.
* Never blindly apply â†’ always review plan.
* If conflict â†’ decide if Terraform state should override or accept infra change.
* Best practice â†’ use CI/CD pipeline with `terraform plan` PR checks + remote backend (S3 + DynamoDB).

ğŸŸ¢ **Story:** â€œWe once found that ops had manually resized an RDS instance in prod. `terraform plan` flagged drift. Instead of blindly applying, I reviewed with DB team and updated Terraform code to match. This avoided accidental DB downtime.â€

---

**2. If EC2 creation failed mid-apply**
ğŸ‘‰ **Headline:** Target + Re-apply

* Use `terraform apply -target=aws_instance.my_ec2`.
* Or `terraform taint` if resource is corrupted â†’ forces re-create.
* Always store state remotely (S3/Dynamo) to avoid mismatch.

ğŸŸ¢ **Story:** â€œIn prod, one EC2 failed due to subnet mismatch. Instead of re-applying whole infra, I targeted just that resource, fixed subnet\_id, and applied cleanly.â€

---

**3. Structuring Terraform modules for multi-AZ**
ğŸ‘‰ **Headline:** DRY + Parametrize + For\_each

* Use a VPC module (networking).
* Pass list of subnets per AZ.
* Use `for_each` to create EC2 per AZ.
* Keep separate `prod`, `stage`, `dev` workspaces.

ğŸŸ¢ **Story:** â€œWe used modules for VPC, EKS, and RDS. For EC2s, we looped over AZs to ensure HA without writing duplicate code.â€

---

**4. Secrets in Terraform**
ğŸ‘‰ **Headline:** Never hardcode â†’ Use Vault/SSM

* Use AWS SSM Parameter Store / Secrets Manager.
* Or HashiCorp Vault integration.
* Mask secrets in `terraform.tfvars`.

ğŸŸ¢ **Story:** â€œWe had DB passwords in plain tfvars â€” security flagged it. Moved to AWS Secrets Manager and pulled dynamically during apply. Passed audit cleanly.â€

---

**5. Multiple teams same Terraform state**
ğŸ‘‰ **Headline:** Remote state + Locking

* Store state in S3 backend with DynamoDB locking.
* Use separate workspaces/envs.
* Use Terragrunt for team collaboration.

ğŸŸ¢ **Story:** â€œTwo teams deploying same infra caused race conditions. Added DynamoDB state lock â€” fixed overlapping apply issues.â€

---

# **AWS & Networking**

**6. EC2 in private subnet â†’ push to S3**
ğŸ‘‰ **Headline:** Role + Gateway + Policy

* EC2 â†’ IAM Role with S3 write policy.
* Route â†’ NAT Gateway (if internet needed) OR VPC Endpoint (best).
* S3 Bucket policy restricts only that role.

ğŸŸ¢ **Story:** â€œWe set up a VPC Gateway Endpoint for S3. That cut NAT traffic cost by \~40% and removed public exposure.â€

---

**7. Public server in private subnet (mistake)**
ğŸ‘‰ **Headline:** 2 Fixes â†’ Move or Re-map

* If possible â†’ stop instance â†’ move ENI to public subnet.
* Else â†’ attach Elastic IP + NAT/ALB.
* Donâ€™t hack security groups; fix subnet design.

ğŸŸ¢ **Story:** â€œSomeone deployed a web server in private subnet. Instead of rebuilding, we migrated ENI to public subnet and re-attached Elastic IP â€” app came back online in minutes.â€

---

**8. EKS pods access S3 without static creds**
ğŸ‘‰ **Headline:** IRSA (IAM Role for Service Accounts)

* Create IAM role with S3 policy.
* Map to K8s ServiceAccount.
* Pods use role automatically.

ğŸŸ¢ **Story:** â€œWe replaced AWS creds in pods with IRSA. This eliminated secret rotation headaches and passed security review.â€

---

**9. ALB vs NLB**
ğŸ‘‰ **Headline:** ALB = HTTP brain, NLB = TCP muscle

* ALB = Layer 7 â†’ routing by hostname/path.
* NLB = Layer 4 â†’ raw performance, static IPs.
* NLB supports TCP/UDP; ALB doesnâ€™t.

ğŸŸ¢ **Story:** â€œWe used ALB for microservices with routing. For gaming TCP traffic, we used NLB. Wrong choice here can kill performance.â€

---

**10. HA in AWS using AZs**
ğŸ‘‰ **Headline:** Spread + Autoheal + DR

* Spread EC2/EKS nodes across â‰¥2 AZs.
* Use Auto Scaling groups.
* RDS Multi-AZ / Aurora.
* S3 cross-region replication for DR.

ğŸŸ¢ **Story:** â€œWe designed 99.99% uptime infra by spreading EKS + RDS across 3 AZs. During AZ outage, traffic shifted automatically.â€

---

# **Kubernetes (EKS)**

**11. Compute engine behind EKS**
ğŸ‘‰ **Headline:** Worker = EC2 (or Fargate)

* Control plane = AWS managed (not visible).
* Worker nodes = EC2 ASG OR Fargate.
* Networking = CNI plugin (ENIs in VPC).

---

**12. Challenges in K8s v1.33 upgrade**
ğŸ‘‰ **Headline:** API Deprecation + PreferClose

* Watch for deprecated APIs (Ingress v1beta1 gone long ago).
* PreferClose scheduling can affect pod placement (closer nodes chosen â†’ imbalance).
* Need to test workloads with new scheduler.

---

**13. Upgrades & PodDisruptionBudget**
ğŸ‘‰ **Headline:** Drain smartly

* `kubectl drain` respects PDB.
* If PDB too strict (minAvailable=100%), no upgrade possible.
* Adjust PDB temporarily.

ğŸŸ¢ **Story:** â€œDuring node upgrade, PDB blocked drain. We temporarily reduced PDB from 100% â†’ 80%, upgraded nodes, then restored. Zero downtime.â€

---

**14. StatefulSet vs DaemonSet vs Deployment**

* Deployment â†’ stateless apps (e.g., web app).
* StatefulSet â†’ DB, Kafka, apps needing persistent ID/storage.
* DaemonSet â†’ 1 pod per node (e.g., logging agent, monitoring agent).

ğŸŸ¢ **Story:** â€œWe ran Fluentd as DaemonSet, Kafka as StatefulSet, and microservices as Deployments.â€

---

**15. Pods running, service unreachable**
ğŸ‘‰ **Checklist:**

* `kubectl describe svc` â†’ check selectors.
* `kubectl get ep` â†’ endpoints exist?
* SG/NACL allow traffic?
* DNS resolving?

ğŸŸ¢ **Story:** â€œApp pods were running but service label selector was wrong (`app=frontend` vs `app=fe`). Corrected selector â†’ traffic started flowing.â€

---

# **CI/CD & Automation**

**16. Multi-env pipeline in Jenkins**
ğŸ‘‰ **Headline:** Params + Branch/Env strategy

* Use Jenkins pipeline parameters (`ENV=dev/prod`).
* Deploy to namespace/env based on param.
* Store values in Helm values.yaml per env.

---

**17. Helm rollback if deploy fails**
ğŸ‘‰ **Headline:** Detect + Rollback

* Use `helm upgrade --atomic`.
* If fail â†’ auto rollback to last good version.
* Jenkins monitors Helm exit code.

ğŸŸ¢ **Story:** â€œOne prod deploy failed due to bad config. Helm atomic rollback restored last good state in <1 min.â€

---

**18. Automated task with Ansible/Python**
ğŸ‘‰ **Headline:** Replace repetitive â†’ Script it

* Ansible: Automated weekend EC2 stop/start by tags.
* Python: Automated EKS node health check + alert to Slack.

ğŸŸ¢ **Story:** â€œWe saved 20+ hours/month by automating EC2 shutdown with Ansible. Finance team loved the cost savings.â€

---

# **Monitoring & Observability**

**19. Use case: Fluentd, Prometheus, Grafana**

* Fluentd â†’ ship logs to S3/Elasticsearch.
* Prometheus â†’ collect pod metrics (CPU/mem).
* Grafana â†’ visualize + alert on dashboards.

---

**20. Grafana Data Source**
ğŸ‘‰ **Headline:** Input for Dashboards

* Examples: Prometheus, CloudWatch, Loki, Elasticsearch, MySQL.
* Iâ€™ve used 3â€“4 in production (Prometheus, CloudWatch, Loki, Elastic).

---

**21. CloudWatch vs Prometheus/Grafana**
ğŸ‘‰ **Headline:** Native vs Open-source power

* CloudWatch â†’ native AWS metrics/logs, easy, but \$\$\$.
* Prometheus/Grafana â†’ fine-grained metrics, flexible queries, but needs infra.

ğŸŸ¢ **Story:** â€œWe used CloudWatch for AWS infra monitoring, but for app-level metrics we needed Prometheus + Grafana because queries were more powerful.â€

---

âœ… Thatâ€™s your **â€œcheat sheetâ€**:

* Headlines â†’ memory hooks
* Bullets â†’ interview structure
* Stories â†’ show real experience

---

ğŸ‘‰ Great question ğŸ‘ Letâ€™s go step by step and make this **interview-ready** with both **process explanation** and a **real-time story** you can tell.

---

## ğŸŒ Terraform Drift Handling in Production

### Headline: **Detect â†’ Review â†’ Decide â†’ Correct â†’ Apply**

---

### ğŸ” Step 1: Detect drift

* Command:

  ```bash
  terraform plan -refresh-only
  ```
* What happens:

  * Terraform refreshes the **state file** with the **real infrastructure** from the cloud provider (e.g., AWS).
  * It does **not plan to change anything** in infra; it just shows differences between **Terraform state** and **actual resources**.
  * Example: An EC2 instance type changed from `t3.medium` â†’ `t3.large` manually.

---

### ğŸ“ Step 2: Review drift

* Carefully look at what has drifted:

  * **Minor metadata** (tags, SG rules) â†’ maybe accidental/manual ops change.
  * **Critical resources** (RDS resize, ASG changes) â†’ could be intentional by another team.

---

### âš–ï¸ Step 3: Decide

Now you have **two choices**:

#### âœ… If you want to **keep the manual change**:

1. **Update Terraform code** to reflect the new reality.
   Example: Change `instance_type = "t3.large"` in Terraform config.
2. Run `terraform plan` again â†’ drift is gone.
3. Apply safely with CI/CD pipeline.

ğŸ‘‰ This ensures **Terraform state = real infra = code**.

---

#### âŒ If you do **NOT want to keep the manual change**:

1. Keep Terraform code as-is.
2. Run normal `terraform plan` â†’ Terraform will propose to revert infra back to the original config.
3. Review â†’ Apply â†’ Infra reverts to match Terraform code.

ğŸ‘‰ This enforces **Infra should always follow code**.

---

### ğŸš€ Step 4: Apply via pipeline

* Never run `terraform apply` directly in prod.
* Use CI/CD pipeline with:

  * Remote backend (S3 + DynamoDB lock).
  * PR checks showing `terraform plan` output.
  * Peer review before merge.

---

## ğŸ”¥ Real-World Story (Interview Style Answer)

*"In one case, our ops team manually resized an RDS instance in production during a performance issue. A week later, when I ran `terraform plan -refresh-only`, it flagged a drift. Instead of blindly applying, I discussed with the DB team and confirmed the change was intentional. I updated the Terraform code to reflect the new size, so our IaC stayed in sync with reality. This avoided accidental downtime if Terraform had tried to revert it. Since then, we enforced a policy: any infra change must go through Terraform PRs, with drift detection as part of our CI/CD checks."*

---

âœ… So your **structured interview answer** would be:

1. Detect drift with `terraform plan -refresh-only`.
2. Review the plan output.
3. If we want to **keep the manual change** â†’ update Terraform code.
4. If we donâ€™t want it â†’ apply Terraform to override manual change.
5. Always go through PR review + remote backend for safe apply.

---


